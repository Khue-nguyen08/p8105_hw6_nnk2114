---
title: "p8105_hw6_nnk"
output: github_document
---

```{r setup , include = FALSE}
library(tidyverse)
library(p8105.datasets)
library(ggplot2)
library(modelr)
library(mgcv)
library(broom)

set.seed(1)
#library(readxl)
#library(haven)
theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1
**import dataset**
```{r, message= FALSE}
homicide_df= 
  read_csv("./data/homicide-data.csv") |>
  janitor::clean_names()

```

**cleaning the data**

```{r, warning=FALSE}
homicide_df = 
  homicide_df |>
   mutate(
  city_state = str_c(city, state, sep = ", "),
  resolved  = if_else(disposition == "Closed by arrest", 1, 0),
  victim_age = as.numeric(victim_age)
  ) |> 
  
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", 
                       "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
  ) |> 
  drop_na(victim_age, victim_sex, victim_race)

```

 **GLM regression**
```{r}
baltimore_df = 
  homicide_df |> 
  filter(city_state == "Baltimore, MD")

baltimore_fit = 
  baltimore_df |> 
  glm(
    resolved ~ victim_age + victim_sex + victim_race,
    data   = _,
    family = binomial()
  )

baltimore_or = 
  baltimore_fit |> 
  tidy(conf.int = TRUE, exponentiate = TRUE) |> 
  filter(term == "victim_sexMale") |> 
  mutate(term = "Male vs Female (ref)") |> 
  select(term, estimate, conf.low, conf.high)

baltimore_or |> 
  knitr::kable(
    col.names = c(
      "Male vs Female", "Odd ratio", "Lower 95%CI", "Upper 95% CI"
    ),
    digits = 3)
```
 
 **GLM regression for all cities**
```{r}
city_glm_df = 
  homicide_df |>
  nest(data = - city_state)|>
  mutate(
    models = map(data,~ glm(resolved ~ victim_age + victim_sex + victim_race,
            data = ., family = binomial())),
    tidy_res = map(models,~ tidy(., conf.int = TRUE, exponentiate = TRUE))
  ) |> 
  select(city_state, tidy_res) |> 
  unnest(tidy_res) |> 
  filter(term == "victim_sexMale") 

city_glm_df |>
   arrange(estimate) |> 
  select(city_state, estimate, conf.low, conf.high) |> 
  mutate(
    estimate       = round(estimate, 3),
    conf.low = round(conf.low, 3),
    conf.high = round(conf.high, 3)
  ) |> 
  knitr::kable(
    col.names = c(
      "City",
      "OR (Male vs Female)",
      "Lower 95% CI",
      "Upper 95% CI"
    )
  )

```
 
 Create a plot to show the estimated OR and 95%CI
 
```{r}
city_glm_df |>
  mutate(city_state = fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point(color = "blue")+
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.5) +
  geom_hline(yintercept = 1, linetype = "dashed", color = "red") +
  coord_flip() +
  labs(
    title = "Odd Ratios for solved homicides (Male vs Female)",
    x = "City",
    y = "Odd Ratio"
  ) +
  theme_minimal()
```

**comment**
The Odd ratios of solving crime for male vs female varied across cities ranged from lowest in NY (0.26, 95% CI : 0.133-0.485) to highest in Albuquerque (1.767, 95% CI : 0.825, 3.762). 
Most cities have OR <1 which indicated the crimes involving female victims were likely to be solved more than the ones with male victims. The 95% CI of these ORs were not including 1 so the differences by victim sex in relation to crime solving status are statistically significant.
Only six ities with ORs > 1 which means at these cities the crimes were more likely in the solved category for male victims compared to female victim cases. However cities with the ORs higher than 1 also have the 95% CI of the odd ratios included 1 which means there is no statistically significant difference between victim sex (male vs female) in correlation with crime solving status. 


## Problem 2
 
**import data**
```{r}
data("weather_df")
set.seed(11)
```
We’ll focus on a simple linear regression with `tmax` as the response with `tmin` and `prcp` as the predictors, and are interested in the distribution of two quantities estimated from these data:

* Estimated R-squared
* Ratio of estimated beta_1/beta_2 (β1/β2)

**cleaning the data**
```{r}
weather_df1 = weather_df |>
  drop_na(prcp, tmax, tmin)
  
```

checking a simple linear fit

```{r}
 fit = lm(tmax ~ tmin+ prcp, data = weather_df1)
summary(fit)
```

bootstrapping function

```{r}
boot_function = function(df) {
  boot_df = df |> 
    sample_frac(size = 1, replace = TRUE)
  fit = lm(tmax ~ tmin + prcp, data = boot_df)
  r_squared = broom::glance(fit) |> 
    pull(r.squared)
  beta_1 = broom::tidy(fit) |> 
    filter(term == "tmin") |> 
    pull(estimate) 
  beta_2 = broom::tidy(fit) |> 
    filter(term == "prcp") |> 
    pull(estimate) 
  beta_ratio = beta_1/beta_2
  
  tibble(r_squared = r_squared, beta_ratio = beta_ratio)
}
```
Testing function
```{r}
boot_function(weather_df1)
```

Create 5,000 bootstrap samples and fit the model in each sample

```{r}
weather_sample = 
  tibble(
    iter = 1:5000
  ) |> 
  mutate(
    sample = map(iter, \(i) boot_function(df = weather_df1))
  ) |> 
  unnest(sample)
```

Plot the distributions

```{r}
weather_sample |> 
  ggplot(aes(x = r_squared)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "white")

weather_sample |> 
  ggplot(aes(x = beta_ratio)) +
  geom_histogram(bins = 30, fill = "lightpink", color = "white") 
```

identify 2.5% and 97.5% quantiles to find 95% confidence intervals for r_squared
```{r}
weather_sample |> 
  summarize(
    r_squared_lower_ci = quantile(r_squared, 0.025),
    r_squared_upper_ci = quantile(r_squared, 0.975),
    beta_ratio_lower = quantile(beta_ratio, 0.025),
    beta_ratio_upper = quantile(beta_ratio, 0.975)
  ) |> 
  knitr::kable(digits = 3)
```

**Comment**
The 95% CI of R-squared are 0.934-0.946 which means the model is consistently explaining more thatn 93% of the variation of `tmax`. The 95% CI of the ratio of  β1/β2 are -277.04, -125.02 which means that `tmin` showed strong possitive association with `tmax` while `prcp` has a weak negative association with `tmax`. 

## Problem 3
Load and clean the data for regression analysis (i.e. use appropriate variable names, convert numeric to factor where appropriate, check for the presence of missing data, etc.).


**immport the data**
```{r}
bw_df = read_csv("./data/birthweight.csv") |> 
  janitor::clean_names()
bw_df |> glimpse()
```

 Propose a regression model for birthweight. This model may be based on a hypothesized structure for the factors that underly birthweight, on a data-driven model-building process, or a combination of the two. Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.
 
 
```{r}
bw_df =
  bw_df |>
  mutate(
    babysex = factor(babysex, labels = c("male", "female")),
    malform = factor(malform, labels = c("absent", "present")),
    mrace   = factor(mrace),
    frace   = factor(frace)
  )
bw_df |> 
  select(bwt, bhead, blength, gaweeks, momage, delwt, smoken, wtgain) |> 
  pivot_longer(everything()) |> 
  ggplot(aes(value)) + 
  geom_histogram(bins = 30) +
  facet_wrap(~ name, scales = "free")
```
 
my model included:
`babysex`, `bhead`, `blength`, `gaweeks`, `momage`, `smoken`, `wtgain` where I also explored the interaction between `gaweeks` and `bhead` and `blength `.

```{r}
mod_1 = bw_df |>
  lm(
    bwt ~ babysex + bhead*blength*gaweeks + momage + smoken + wtgain,
    data = _
  )

mod_1 |> tidy()

bw_df_add = bw_df |>
  add_predictions(mod_1) |>
  add_residuals(mod_1)


bw_df_add |>
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3, color = "skyblue") +
  geom_hline(yintercept = 0, linetype = "dashed") +
  labs(
    title = "Residuals vs Fitted Values for Proposed Birthweight Model",
    x = "Fitted birthweight (grams)",
    y = "Residuals"
  ) +
  theme_minimal()
```

 
**Compare your model to two others**
```{r}
#main effect only
mod_2 = bw_df |>
  lm(bwt ~ + blength + gaweeks, data =_)
mod_2 |> tidy()

mod_3 = bw_df |>
  lm(bwt ~ bhead * blength * babysex, data = _)
mod_3|> tidy()

```

**Cross validation**
```{r}
set.seed(123)

cv_df = 
  crossv_mc(bw_df, n = 100)

cv_results =
  cv_df |> 
  mutate(
    mod_1 = map(train, ~ lm(bwt ~ babysex + bhead*blength*gaweeks + momage + smoken + wtgain, data = .x)),
    mod_2 = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    mod_3 = map(train, ~ lm(bwt ~ bhead * blength * babysex, data = .x)),
    
    rmse_1 = map2_dbl(mod_1, test, ~ rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(mod_2, test, ~ rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(mod_3, test, ~ rmse(model = .x, data = .y))
  )
# table for mean rmse
cv_results |>
  summarize(
    rmse_1 = mean(rmse_1),
    rmse_2 = mean(rmse_2),
    rmse_3 = mean(rmse_3)
  ) |> 
  knitr::kable(digits = 3,
  col.names = c("Proposed model",
              "Main effect only",
              "2- and 3-way interactions of head x length x sex"
                ))
# view the variation of the rmse
cv_results |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_boxplot()

```

**Comment**
Root Mean Squared Error (RMSE) mean and variation showed that the main effect model had the highest errors, eman rsme equal 330.7, compared to the proposed model and the interaction models. The interaction model showed lower RSME value with mean at 288.385. The proposed model:  lm(bwt ~ babysex + bhead*blength*gaweeks + momage + smoken + wtgain) showed the lowest RSME valued, 281. 


